# ğŸ§ª SAP Test Case Generator AI Agent

## Overview

This project implements an AI-powered agent that generates edge-to-edge test cases from uploaded SAP FDS (Functional Design Specifications) or TDS (Technical Design Specifications) documents. It leverages Retrieval-Augmented Generation (RAG), web search, conversational memory, feedback loops, MongoDB logging, and a LangGraph multi-agent pipeline to ensure robust, accurate, and explainable outputs.

---

## ğŸ”§ Features

* ğŸ” **Document-based Retrieval**: Vector store (HuggingFace + DocArrayInMemorySearch) based PDF chunk retrieval.
* ğŸŒ **Web Search Integration**: Using Tavily API for external signal augmentation.
* ğŸ’¬ **Conversational Memory**: Retains interaction history with `ConversationBufferMemory`.
* ğŸ” **Feedback Loop**: Self-scoring + answer retry if score < 7.
* ğŸ§  **Multi-Agent Workflow**: LangGraph pipeline with nodes for retrieval, answer generation, explanation, scoring.
* ğŸ“Š **Logging**: MongoDB session logging for traceability.
* ğŸ›ï¸ **Streamlit UI**: Chat interface for PDF upload and prompt-based interaction.

---

## ğŸ› ï¸ Setup Instructions

### 1. Environment Setup

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Required API Keys (Set as Environment Variables)

* `TOGETHER_API_KEY` â€“ for Mixtral model via Together
* `TAVILY_API_KEY` â€“ for web search

You can export them in your shell or `.env`:

```bash
export TOGETHER_API_KEY="<your-key>"
export TAVILY_API_KEY="<your-key>"
```

### 3. MongoDB

* Start local instance at `mongodb://localhost:27017`
* Create database `sap_testcase_gen` and collection `session_logs`

### 4. Run the App

```bash
streamlit run test_case_generating_agent.py
```

---

## ğŸ“„ Sample Prompts

* "Generate test cases for the Purchase Order creation flow."
* "Create edge scenarios for invoice verification failure."
* "What are the test cases for incorrect tax logic in FDS?"

---

## ğŸ“š Techniques Implemented

### âœ… From Research:

* **Corrective RAG**: Retry answer if feedback score < 7
* **Self RAG**: Explains reasoning of output
* **Adaptive RAG**: Temperature tuning based on score feedback
* **LangGraph Orchestration**: StateGraph pipeline with edge routing
* **Web-Augmented RAG**: Uses Tavily for external augmentation
* **Memory-Augmented Chat**: Maintains dialogue context

---

## ğŸ§© Agent Pipeline Architecture

```
Upload PDF
   |
[Retriever] <-----
   |             |
[Web Search]    [Memory Save]
   |             |
[LLM Answer Generation]
   | --> [LLM Explanation Node]
            |
     [Scoring Agent (LLM)]
        |
 [Retry if score < 7]
        |
       END
```

---

## ğŸ“ File Structure

```
â”œâ”€â”€ test_case_generating_agent.py  # Streamlit app & LangGraph agent
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ README.md                      # This file
```

---

## ğŸ“Œ Future Enhancements

* âœ… Agent scoring visualization in UI
* âœ… Role-based prompt routing
* âœ… Tabular document RAG with layout-aware retrieval
* âœ… Integration with other SAP agents (FDS â†” TDS â†” Test Case â†” ABAP)
* âœ… Prompt suggestions as UI buttons

---

## ğŸ‘¨â€ğŸ’» Author

Developed by \[Your Name / Team] as part of an intelligent SAP automation suite using LangGraph and Agentic AI principles.

---

## ğŸ“œ License

MIT License
